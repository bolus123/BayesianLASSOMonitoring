"phibound0" = Inf,
"phiboundqplus1" = 0,
"betaA" = diag(nrow = 1),
"gammaxi2" = 0.1,
"tautheta1" = 1,
"tautheta2" = 1,
"sigma2a" = 1,
"sigma2b" = 1,
"updatelambda2" = 1,
"lambda2alpha" = 1,
"lambda2beta" = 1,
"YJ" = 1,
"updateYJ" = 1,
"leftcensoring" = 1,
"lowerbound" = 0,
"rounding" = 1
)
qq <- BayesianLASSOMonitoring::GibbsRFLSMX(matrix(Y1, ncol = 1), bset, H = H1,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1)
library(BayesianLASSOMonitoring)
GibbsRFLSMX(matrix(Y1, ncol = 1), bset, H = H1,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1)
GibbsRFLSMXcpp(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1, lambda2 = matrix(rep(1e-6, 5), ncol = 1), theta = 1)
seed <- 12345
alpha <- c(0.03083069, 0.06242601, 0.09120189)
lambda <- 0.239385
pi <- 0.1453097
nnsim <- 100
TT1 <- c(92, 183, 365)
#TT1 <- c(92)
TT2 <- 365
w <- c(1)
#w <- 7
q <- 5
delta <- c(0, 0.5, 1)
tt <- 0.8
Y.hat.method <- c("median")
side <- "right-sided"
FAP0 <- 0.2
log <- c(FALSE)
sta <- c(FALSE)
pars <- expand.grid(TT1, TT2, w, q, delta, tt, Y.hat.method, side, FAP0, log, sta, 1:nnsim)
X <- 9
set.seed(seed + X)
TT1 <- pars[X, 1]
TT2 <- pars[X, 2]
w <- pars[X, 3]
q <- pars[X, 4]
delta <- pars[X, 5]
tt <- pars[X, 6]
Y.hat.method <- as.character(pars[X, 7])
side <- as.character(pars[X, 8])
FAP0 <- pars[X, 9]
log <- pars[X, 10]
sta <- pars[X, 11]
Y <- BayesianLASSOMonitoring::rzinpoisinar3(TT1 + TT2 + w - 1, alpha, lambda, pi,
ceiling(TT1 * tt) + w - 1, delta = delta, burnin = 100)
Y0 <- Y[1:(w - 1)]
Y1 <- Y[w:(TT1 + w - 1)]
Y2 <- Y[(TT1 + w):(TT1 + TT2 + w - 1)]
#H1 <- BayesianLASSOMonitoring::getHMatMT(TT1, q)
#H2 <- matrix(1, nrow = TT2, ncol = TT1 - q)
#H2sim <- matrix(1, nrow = 5000, ncol = TT1 - q)
H1 <- BayesianLASSOMonitoring::getHMatMT(TT1, q)
H1 <- H1[, seq(1, TT1 - q, 7)]
H1 <- H1[, -c(colSums(H1) < 7)]
H2<- matrix(1, nrow = TT2, ncol = dim(H1)[2])
H2sim <- matrix(1, nrow = 5000, ncol = dim(H1)[2])
##############################################
bset <- list(
"method" = "ALASSO",
"phimono" = 1,
"phiq" = 5,
"phiA" = diag(nrow = 5),
"phibound0" = Inf,
"phiboundqplus1" = 0,
"betaA" = diag(nrow = 1),
"gammaxi2" = 0.1,
"tautheta1" = 1,
"tautheta2" = 1,
"sigma2a" = 1,
"sigma2b" = 1,
"updatelambda2" = 1,
"lambda2alpha" = 1,
"lambda2beta" = 1,
"YJ" = 1,
"updateYJ" = 1,
"leftcensoring" = 1,
"lowerbound" = 0,
"rounding" = 1,
"lambda2" = NULL,
"theta" = NULL
)
qq <- GibbsRFLSMX(matrix(Y1, ncol = 1), bset, H = H1,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1)
qq1 <- GibbsRFLSMXcpp(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1)
remove.packages("BayesianLASSOMonitoring")
devtools::install_github("bolus123/BayesianLASSOMonitoring", "trans-and-x")
seed <- 12345
alpha <- c(0.03083069, 0.06242601, 0.09120189)
lambda <- 0.239385
pi <- 0.1453097
nnsim <- 100
TT1 <- c(92, 183, 365)
#TT1 <- c(92)
TT2 <- 365
w <- c(1)
#w <- 7
q <- 5
delta <- c(0, 0.5, 1)
tt <- 0.8
Y.hat.method <- c("median")
side <- "right-sided"
FAP0 <- 0.2
log <- c(FALSE)
sta <- c(FALSE)
pars <- expand.grid(TT1, TT2, w, q, delta, tt, Y.hat.method, side, FAP0, log, sta, 1:nnsim)
X <- 9
set.seed(seed + X)
TT1 <- pars[X, 1]
TT2 <- pars[X, 2]
w <- pars[X, 3]
q <- pars[X, 4]
delta <- pars[X, 5]
tt <- pars[X, 6]
Y.hat.method <- as.character(pars[X, 7])
side <- as.character(pars[X, 8])
FAP0 <- pars[X, 9]
log <- pars[X, 10]
sta <- pars[X, 11]
Y <- BayesianLASSOMonitoring::rzinpoisinar3(TT1 + TT2 + w - 1, alpha, lambda, pi,
ceiling(TT1 * tt) + w - 1, delta = delta, burnin = 100)
Y0 <- Y[1:(w - 1)]
Y1 <- Y[w:(TT1 + w - 1)]
Y2 <- Y[(TT1 + w):(TT1 + TT2 + w - 1)]
#H1 <- BayesianLASSOMonitoring::getHMatMT(TT1, q)
#H2 <- matrix(1, nrow = TT2, ncol = TT1 - q)
#H2sim <- matrix(1, nrow = 5000, ncol = TT1 - q)
H1 <- BayesianLASSOMonitoring::getHMatMT(TT1, q)
H1 <- H1[, seq(1, TT1 - q, 7)]
H1 <- H1[, -c(colSums(H1) < 7)]
H2<- matrix(1, nrow = TT2, ncol = dim(H1)[2])
H2sim <- matrix(1, nrow = 5000, ncol = dim(H1)[2])
##############################################
bset <- list(
"method" = "ALASSO",
"phimono" = 1,
"phiq" = 5,
"phiA" = diag(nrow = 5),
"phibound0" = Inf,
"phiboundqplus1" = 0,
"betaA" = diag(nrow = 1),
"gammaxi2" = 0.1,
"tautheta1" = 1,
"tautheta2" = 1,
"sigma2a" = 1,
"sigma2b" = 1,
"updatelambda2" = 1,
"lambda2alpha" = 1,
"lambda2beta" = 1,
"YJ" = 1,
"updateYJ" = 1,
"leftcensoring" = 1,
"lowerbound" = 0,
"rounding" = 1,
"lambda2" = NULL,
"theta" = NULL
)
qq <- BayesianLASSOMonitoring::GibbsRFLSMX(matrix(Y1, ncol = 1), bset, H = H1,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1)
qq <- BayesianLASSOMonitoring::GibbsRFLSMX(matrix(Y1, ncol = 1), bset, H = H1,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1)
qq1 <- BayesianLASSOMonitoring:GibbsRFLSMXcpp(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1)
qq1 <- BayesianLASSOMonitoring::GibbsRFLSMXcpp(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1)
qq1 <- BayesianLASSOMonitoring::GibbsRFLSMXcpp(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1)
qq1 <- BayesianLASSOMonitoring::GibbsRFLSMXcpp(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1)
qq1 <- BayesianLASSOMonitoring::GibbsRFLSMXcpp(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1)
qq1 <- BayesianLASSOMonitoring::GibbsRFLSMXcpp(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1)
Rcpp::sourceCpp("src/BayesianLASSOMonitoring.cpp")
Rcpp::sourceCpp("src/BayesianLASSOMonitoring.cpp")
qq2 <- GibbsRFLSMXcpp(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1)
Ph1MultipleTesting.Y01 <- function(model, bset,
FAP0 = 0.2, side = "right-sided",
nsim = 10000, interval = c(0.000001, 0.499999)) {
root.finding <- function(adj.alpha, ph1mat, FAP0, n, nsim, side = "right-sided") {
lim <- matrix(NA, nrow = n, ncol = 2)
sig <- matrix(NA, nrow = n, ncol = nsim)
for (i in 1:n) {
if (side == "right-sided") {
lim[i, 1] <- -Inf
lim[i, 2] <- quantile(ph1mat[i, ], 1 - adj.alpha)
} else if (side == "left-sided") {
lim[i, 1] <- quantile(ph1mat[i, ], adj.alpha)
lim[i, 2] <- infert
} else if (side == "two-sided") {
lim[i, 1] <- quantile(ph1mat[i, ], adj.alpha / 2)
lim[i, 2] <- quantile(ph1mat[i, ], 1 - adj.alpha / 2)
}
}
for (i in 1:nsim) {
sig[, i] <- (lim[, 1] <= ph1mat[, i]) & (ph1mat[, i] <= lim[, 2])
}
tmp <- mean(colSums(sig) == n)
dif <- tmp - (1 - FAP0)
##cat("dif:", dif, "\n")
return(dif)
}
q <- dim(model$Phi)[1]
n <- length(model$Y)
nnsim <- dim(model$Phi)[2]
ph1mat <- matrix(NA, nrow = n, ncol = nsim)
for (i in 1:nsim) {
tmpsel <- sample(1:nnsim, 1)
Mu0 <- matrix(rep(model$mu0[tmpsel], n))
if (!is.null(model$X)) {
Mu0 <- Mu0 + model$X %*% (model$Beta[, tmpsel])
}
tmpYyj <- yeojohnsontr(model$Z[, tmpsel] + model$Y, model$theta[tmpsel], 1e-32)
ph1mat[, i] <- simYXph1(matrix(tmpYyj, ncol = 1), matrix(model$Phi[, tmpsel], ncol = 1), Mu0,
model$sigma2[tmpsel],  model$theta[tmpsel],
1e-32, bset$leftcensoring, bset$lowerbound, bset$rounding)
}
adj.alpha <- uniroot(root.finding, interval, ph1mat = ph1mat, FAP0 = FAP0, n = n, nsim = nsim, side = side,
tol = 1e-6)$root
lim <- matrix(NA, nrow = n, ncol = 2)
sig <- matrix(NA, nrow = n, ncol = 1)
for (i in 1:n) {
if (side == "right-sided") {
lim[i, 1] <- -Inf
lim[i, 2] <- quantile(ph1mat[i, ], 1 - adj.alpha, na.rm = TRUE)
} else if (side == "left-sided") {
lim[i, 1] <- quantile(ph1mat[i, ], adj.alpha, na.rm = TRUE)
lim[i, 2] <- infert
} else if (side == "two-sided") {
lim[i, 1] <- quantile(ph1mat[i, ], adj.alpha / 2, na.rm = TRUE)
lim[i, 2] <- quantile(ph1mat[i, ], 1 - adj.alpha / 2, na.rm = TRUE)
}
}
sig <- 1 - ((lim[, 1] <= model$Y[-c(1:q)]) & (model$Y[-c(1:q)] <= lim[, 2]))
list("grandsig" = sum(sig) > 0, "sig" = sig, "lim" = lim, "adj.alpha" = adj.alpha,
"Yph1" = ph1mat)
}
debug(Ph1MultipleTesting.Y01)
Ph1MultipleTesting.Y01(qq2, bset,
FAP0 = 0.2, side = "right-sided",
nsim = 10000, interval = c(0.000001, 0.499999))
simYXph1(matrix(tmpYyj, ncol = 1), matrix(model$Phi[, tmpsel], ncol = 1), Mu0,
model$sigma2[tmpsel],  model$theta[tmpsel],
1e-32, bset$leftcensoring, bset$lowerbound, bset$rounding)
tmpYyj
yeojohnsontr(model$Z[, tmpsel] + model$Y, model$theta[tmpsel], 1e-32)
model$Z[, tmpsel] + model$Y
Mu0
model$Phi
Mu0
Mu0 <- matrix(rep(model$mu0[tmpsel], n))
Mu0
length(Mu0)
tmpsel
model$mu0[tmpsel]
rep(model$mu0[tmpsel], n)
length(model$Y)
dim(model$Y)
model$Y
#' Random Flexible Level Shift Model
#'
#' gets a posterior sample using Gibbs sampling for Random Flexible Level Shift Model
#' @param Y is a vector.
#' @param H is the design matrix for shifts.
#' @param X is the input matrix
#' @param q is the number of lags.
#' @param A is a given variance-covariance matrix in MT and regression for the slab-and-spike coefficients.
#' @param a is a given shape of the prior gamma distribution for sigma2.
#' @param b is a given scale of the prior gamma distribution for sigma2.
#' @param alpha is a given shape of the prior gamma distribution for lambda2.
#' @param beta is a given scale of the prior gamma distribution for lambda2.
#' @param theta1 is a given shape1 of the prior beta distribution for the probability of Tau and Kappa.
#' @param theta2 is a given shape2 of the prior beta distribution for the probability of Tau and Kappa.
#' @param xi2 is a given variance of the prior normal distribution for shifts.
#' @param method is a choice of methods including MT(McCulloch-Tsay), regression, LASSO, ALASSO(Adaptive LASSO), MonoLASSO(LASSO with Monotonicity constrains), MonoALASSO(Adaptive LASSO with Monotonicity constrains).
#' @param bound0 is an upper bound of the methods with Monotonicity constrains.
#' @param boundqplus1 is  a lower bound of the methods with Monotonicity constrains.
#' @param nsim is the number of draws from MCMC.
#' @param by is the interval of systematic sampling for the draws from MCMC.
#' @param burnin is the length of burn-in period.
#' @param tol is the tolerance level.
#' @references McCulloch, R. E., & Tsay, R. S. (1993). Bayesian inference and prediction for mean and variance shifts in autoregressive time series. Journal of the american Statistical association, 88(423), 968-978.
#'
#'
#' @export
#' @examples
#' nsim <- 100
#' burnin <- 100
#' T <- 100
#' q <- 5
#' H <- getHMatMT(T, q)
#' Y <- arima.sim(list(ar = 0.5), n = T)
#'
#' result <- GibbsRFLSM(Y, H = H, q = q, nsim = nsim, burnin = burnin)
#'
GibbsRFLSMX <- function(Y, bset, X = NULL, H = NULL,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = TRUE) {
model <- GibbsRFLSMXcpp(matrix(Y, ncol = 1), bset,
tol = tol, nsim = nsim, thin = thin, burnin = burnin, verbose = verbose,
X = X, H = H1)
out <- list(
"Phi" = matrix(model$Phi, ncol = nsim),
"Beta" = matrix(model$Beta, ncol = nsim),
"Gamma" = matrix(model$Gamma, ncol = nsim),
"Tau" = matrix(model$Tau, ncol = nsim),
"mu0" = model$mu0,
"sigma2" = model$sigma2,
"lambda2" = matrix(model$lambda2, ncol = nsim),
"theta" = model$theta,
"Z" = model$Z,
"H" = H,
"X" = X,
"Y" = Y,
"nsim" = nsim
)
return(out)
}
qq2 <- GibbsRFLSMX(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1)
qq3 <- GibbsRFLSMX(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1)
debug(Ph1MultipleTesting.Y01)
Ph1MultipleTesting.Y01(qq3, bset,
FAP0 = 0.2, side = "right-sided",
nsim = 10000, interval = c(0.000001, 0.499999))
Mu0
tmpYyj
simYXph1(matrix(tmpYyj, ncol = 1), matrix(model$Phi[, tmpsel], ncol = 1), Mu0,
model$sigma2[tmpsel],  model$theta[tmpsel],
1e-32, bset$leftcensoring, bset$lowerbound, bset$rounding)
adj.alpha
sig
lim
lim
Ph1MultipleTesting.Y01 <- function(model, bset,
FAP0 = 0.2, side = "right-sided",
nsim = 10000, interval = c(0.000001, 0.499999)) {
root.finding <- function(adj.alpha, ph1mat, FAP0, n, nsim, side = "right-sided") {
lim <- matrix(NA, nrow = n, ncol = 2)
sig <- matrix(NA, nrow = n, ncol = nsim)
for (i in 1:n) {
if (side == "right-sided") {
lim[i, 1] <- -Inf
lim[i, 2] <- quantile(ph1mat[i, ], 1 - adj.alpha)
} else if (side == "left-sided") {
lim[i, 1] <- quantile(ph1mat[i, ], adj.alpha)
lim[i, 2] <- infert
} else if (side == "two-sided") {
lim[i, 1] <- quantile(ph1mat[i, ], adj.alpha / 2)
lim[i, 2] <- quantile(ph1mat[i, ], 1 - adj.alpha / 2)
}
}
for (i in 1:nsim) {
sig[, i] <- (lim[, 1] <= ph1mat[, i]) & (ph1mat[, i] <= lim[, 2])
}
tmp <- mean(colSums(sig) == n)
dif <- tmp - (1 - FAP0)
##cat("dif:", dif, "\n")
return(dif)
}
q <- dim(model$Phi)[1]
n <- length(model$Y)
nnsim <- dim(model$Phi)[2]
ph1mat <- matrix(NA, nrow = n, ncol = nsim)
for (i in 1:nsim) {
tmpsel <- sample(1:nnsim, 1)
Mu0 <- matrix(rep(model$mu0[tmpsel], n))
if (!is.null(model$X)) {
Mu0 <- Mu0 + model$X %*% (model$Beta[, tmpsel])
}
tmpYyj <- yeojohnsontr(model$Z[, tmpsel] + model$Y, model$theta[tmpsel], 1e-32)
ph1mat[, i] <- simYXph1(matrix(tmpYyj, ncol = 1), matrix(model$Phi[, tmpsel], ncol = 1), Mu0,
model$sigma2[tmpsel],  model$theta[tmpsel],
1e-32, bset$leftcensoring, bset$lowerbound, bset$rounding)
}
adj.alpha <- uniroot(root.finding, interval, ph1mat = ph1mat, FAP0 = FAP0, n = n, nsim = nsim, side = side,
tol = 1e-6)$root
lim <- matrix(NA, nrow = n, ncol = 2)
sig <- matrix(NA, nrow = n, ncol = 1)
for (i in 1:n) {
if (side == "right-sided") {
lim[i, 1] <- -Inf
lim[i, 2] <- quantile(ph1mat[i, ], 1 - adj.alpha, na.rm = TRUE)
} else if (side == "left-sided") {
lim[i, 1] <- quantile(ph1mat[i, ], adj.alpha, na.rm = TRUE)
lim[i, 2] <- infert
} else if (side == "two-sided") {
lim[i, 1] <- quantile(ph1mat[i, ], adj.alpha / 2, na.rm = TRUE)
lim[i, 2] <- quantile(ph1mat[i, ], 1 - adj.alpha / 2, na.rm = TRUE)
}
}
sig <- 1 - ((lim[, 1] <= model$Y) & (model$Y <= lim[, 2]))
list("grandsig" = sum(sig) > 0, "sig" = sig, "lim" = lim, "adj.alpha" = adj.alpha,
"Yph1" = ph1mat)
}
qq3 <- GibbsRFLSMX(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1)
debug(Ph1MultipleTesting.Y01)
Ph1MultipleTesting.Y01(qq3, bset,
FAP0 = 0.2, side = "right-sided",
nsim = 10000, interval = c(0.000001, 0.499999))
sig
sig
as.vector(sig)
sum(sig)
plot(Y )
plot(Y1)
plot(model$Y)
model$Phi[1, ]
hist(model$Phi[1, ])
hist(model$Phi[2, ])
hist(model$Phi[3, ])
seed <- 12345
alpha <- c(0.03083069, 0.06242601, 0.09120189)
lambda <- 0.239385
pi <- 0.1453097
nnsim <- 100
TT1 <- c(92, 183, 365)
#TT1 <- c(92)
TT2 <- 365
w <- c(1)
#w <- 7
q <- 5
delta <- c(0, 0.5, 1)
tt <- 0.8
Y.hat.method <- c("median")
side <- "right-sided"
FAP0 <- 0.2
log <- c(FALSE)
sta <- c(FALSE)
pars <- expand.grid(TT1, TT2, w, q, delta, tt, Y.hat.method, side, FAP0, log, sta, 1:nnsim)
X <- 9
set.seed(seed + X)
TT1 <- pars[X, 1]
TT2 <- pars[X, 2]
w <- pars[X, 3]
q <- pars[X, 4]
delta <- pars[X, 5]
tt <- pars[X, 6]
Y.hat.method <- as.character(pars[X, 7])
side <- as.character(pars[X, 8])
FAP0 <- pars[X, 9]
log <- pars[X, 10]
sta <- pars[X, 11]
Y <- BayesianLASSOMonitoring::rzinpoisinar3(TT1 + TT2 + w - 1, alpha, lambda, pi,
ceiling(TT1 * tt) + w - 1, delta = delta, burnin = 100)
Y0 <- Y[1:(w - 1)]
Y1 <- Y[w:(TT1 + w - 1)]
Y2 <- Y[(TT1 + w):(TT1 + TT2 + w - 1)]
#H1 <- BayesianLASSOMonitoring::getHMatMT(TT1, q)
#H2 <- matrix(1, nrow = TT2, ncol = TT1 - q)
#H2sim <- matrix(1, nrow = 5000, ncol = TT1 - q)
H1 <- BayesianLASSOMonitoring::getHMatMT(TT1, q)
#H1 <- H1[, seq(1, TT1 - q, 7)]
#H1 <- H1[, -c(colSums(H1) < 7)]
H2<- matrix(1, nrow = TT2, ncol = dim(H1)[2])
H2sim <- matrix(1, nrow = 5000, ncol = dim(H1)[2])
bset <- list(
"method" = "ALASSO",
"phimono" = 1,
"phiq" = 5,
"phiA" = diag(nrow = 5),
"phibound0" = Inf,
"phiboundqplus1" = 0,
"betaA" = diag(nrow = 1),
"gammaxi2" = 0.1,
"tautheta1" = 1,
"tautheta2" = 1,
"sigma2a" = 1,
"sigma2b" = 1,
"updatelambda2" = 1,
"lambda2alpha" = 1,
"lambda2beta" = 1,
"YJ" = 1,
"updateYJ" = 1,
"leftcensoring" = 1,
"lowerbound" = 0,
"rounding" = 1,
"lambda2" = NULL,
"theta" = NULL
)
qq3 <- GibbsRFLSMX(matrix(Y1, ncol = 1), bset,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = 1,
H = H1)
Rcpp::compileAttributes()
roxygen2::roxygenise()
