1e-6,
X)
}
iter
Rcpp::sourceCpp("src/BayesianLASSOMonitoring.cpp")
iter <- initGibbsRFLSMXcpp(Yyj, bset, 1e-6, X)
for (i in 1:10000) {
iter <- GibbsRFLSMXUpdatecpp(Yyj, iter, bset,
1e-6,
X)
}
iter
iter <- initGibbsRFLSMXcpp(Yyj, bset, 1e-6, X)
iter
bset$lambda2
Rcpp::sourceCpp("src/BayesianLASSOMonitoring.cpp")
X <- 1:300
n <- 300
innov <- rnorm(n)
innov <- innov + X * 0.0025
Y <- arima.sim(list(ar = c(0.5, 0.2, 0.1)), n = n, innov = innov)
Y[151:200] <- Y[151:200] + 1
H <- getHMatMT(n, 3)
X <- as.matrix(X)
Y <- as.matrix(Y)
pars <- list(
"Phi" = as.matrix(c(0.25, 0.1, 0.05)),
"Beta" = as.matrix(0.00125),
"Tau" = as.matrix(rep(0, 97)),
"Gamma" = as.matrix(rnorm(97, 0, sqrt(0.1))),
"mu0" = 0,
"Mu" = 0 + X %*% 0.00125,
"eta2" = as.matrix(c(c(c(0.25, 0.1, 0.05)) ^ 2, c(0.00125) ^ 2)),
"sigma2" = 1,
"lambda2" = as.matrix((1 / c(0.25, 0.1, 0.05, 0.00125)) ^ 2),
"theta" = 1
)
bset <- list(
"method" = "MT",
"phimono" = 1,
"phiq" = 5,
"phiA" = diag(nrow = 5),
"phibound0" = Inf,
"phiboundqplus1" = 0,
"betaA" = diag(nrow = 1),
"gammaxi2" = 0.1,
"tautheta1" = 1,
"tautheta2" = 1,
"sigma2a" = 1,
"sigma2b" = 1,
"updatelambda2" = 1,
"lambda2alpha" = 1,
"lambda2beta" = 1,
"updateYJ" = 1,
"leftcensoring" = 1,
"lowerbound" = 0,
"rounding" = 1,
"lambda2" = NULL,
"theta" = NULL
)
##############################################
lambda2 <- pars$lambda2
theta <- pars$theta
Yyj <- yeojohnsontr(Y, theta, 1e-6)
iter <- initGibbsRFLSMXcpp(Yyj, bset, 1e-6, X)
for (i in 1:10000) {
iter <- GibbsRFLSMXUpdatecpp(Yyj, iter, bset,
1e-6,
X)
}
iter
Rcpp::sourceCpp("src/BayesianLASSOMonitoring.cpp")
Rcpp::sourceCpp("src/BayesianLASSOMonitoring.cpp")
X <- 1:300
n <- 300
innov <- rnorm(n)
innov <- innov + X * 0.0025
Y <- arima.sim(list(ar = c(0.5, 0.2, 0.1)), n = n, innov = innov)
Y[151:200] <- Y[151:200] + 1
H <- getHMatMT(n, 3)
X <- as.matrix(X)
Y <- as.matrix(Y)
pars <- list(
"Phi" = as.matrix(c(0.25, 0.1, 0.05)),
"Beta" = as.matrix(0.00125),
"Tau" = as.matrix(rep(0, 97)),
"Gamma" = as.matrix(rnorm(97, 0, sqrt(0.1))),
"mu0" = 0,
"Mu" = 0 + X %*% 0.00125,
"eta2" = as.matrix(c(c(c(0.25, 0.1, 0.05)) ^ 2, c(0.00125) ^ 2)),
"sigma2" = 1,
"lambda2" = as.matrix((1 / c(0.25, 0.1, 0.05, 0.00125)) ^ 2),
"theta" = 1
)
bset <- list(
"method" = "MT",
"phimono" = 1,
"phiq" = 5,
"phiA" = diag(nrow = 5),
"phibound0" = Inf,
"phiboundqplus1" = 0,
"betaA" = diag(nrow = 1),
"gammaxi2" = 0.1,
"tautheta1" = 1,
"tautheta2" = 1,
"sigma2a" = 1,
"sigma2b" = 1,
"updatelambda2" = 1,
"lambda2alpha" = 1,
"lambda2beta" = 1,
"updateYJ" = 1,
"leftcensoring" = 1,
"lowerbound" = 0,
"rounding" = 1,
"lambda2" = NULL,
"theta" = NULL
)
##############################################
lambda2 <- pars$lambda2
theta <- pars$theta
Yyj <- yeojohnsontr(Y, theta, 1e-6)
iter <- initGibbsRFLSMXcpp(Yyj, bset, 1e-6, X)
for (i in 1:10000) {
iter <- GibbsRFLSMXUpdatecpp(Yyj, iter, bset,
1e-6,
X)
}
iter
Rcpp::sourceCpp("src/BayesianLASSOMonitoring.cpp")
X <- 1:300
n <- 300
innov <- rnorm(n)
innov <- innov + X * 0.0025
Y <- arima.sim(list(ar = c(0.5, 0.2, 0.1)), n = n, innov = innov)
Y[151:200] <- Y[151:200] + 1
H <- getHMatMT(n, 3)
X <- as.matrix(X)
Y <- as.matrix(Y)
pars <- list(
"Phi" = as.matrix(c(0.25, 0.1, 0.05)),
"Beta" = as.matrix(0.00125),
"Tau" = as.matrix(rep(0, 97)),
"Gamma" = as.matrix(rnorm(97, 0, sqrt(0.1))),
"mu0" = 0,
"Mu" = 0 + X %*% 0.00125,
"eta2" = as.matrix(c(c(c(0.25, 0.1, 0.05)) ^ 2, c(0.00125) ^ 2)),
"sigma2" = 1,
"lambda2" = as.matrix((1 / c(0.25, 0.1, 0.05, 0.00125)) ^ 2),
"theta" = 1
)
bset <- list(
"method" = "MT",
"phimono" = 1,
"phiq" = 5,
"phiA" = diag(nrow = 5),
"phibound0" = Inf,
"phiboundqplus1" = 0,
"betaA" = diag(nrow = 1),
"gammaxi2" = 0.1,
"tautheta1" = 1,
"tautheta2" = 1,
"sigma2a" = 1,
"sigma2b" = 1,
"updatelambda2" = 1,
"lambda2alpha" = 1,
"lambda2beta" = 1,
"updateYJ" = 1,
"leftcensoring" = 1,
"lowerbound" = 0,
"rounding" = 1,
"lambda2" = NULL,
"theta" = NULL
)
##############################################
lambda2 <- pars$lambda2
theta <- pars$theta
Yyj <- yeojohnsontr(Y, theta, 1e-6)
iter <- initGibbsRFLSMXcpp(Yyj, bset, 1e-6, X)
for (i in 1:10000) {
iter <- GibbsRFLSMXUpdatecpp(Yyj, iter, bset,
1e-6,
X)
}
X <- 1:300
n <- 300
innov <- rnorm(n)
innov <- innov + X * 0.0025
Y <- arima.sim(list(ar = c(0.5, 0.2, 0.1)), n = n, innov = innov)
Y[151:200] <- Y[151:200] + 1
H <- getHMatMT(n, 3)
X <- as.matrix(X)
Y <- as.matrix(Y)
pars <- list(
"Phi" = as.matrix(c(0.25, 0.1, 0.05)),
"Beta" = as.matrix(0.00125),
"Tau" = as.matrix(rep(0, 97)),
"Gamma" = as.matrix(rnorm(97, 0, sqrt(0.1))),
"mu0" = 0,
"Mu" = 0 + X %*% 0.00125,
"eta2" = as.matrix(c(c(c(0.25, 0.1, 0.05)) ^ 2, c(0.00125) ^ 2)),
"sigma2" = 1,
"lambda2" = as.matrix((1 / c(0.25, 0.1, 0.05, 0.00125)) ^ 2),
"theta" = 1
)
bset <- list(
"method" = "MT",
"phimono" = 1,
"phiq" = 5,
"phiA" = diag(nrow = 5),
"phibound0" = Inf,
"phiboundqplus1" = 0,
"betaA" = diag(nrow = 1),
"gammaxi2" = 0.1,
"tautheta1" = 1,
"tautheta2" = 1,
"sigma2a" = 1,
"sigma2b" = 1,
"updatelambda2" = 1,
"lambda2alpha" = 1,
"lambda2beta" = 1,
"updateYJ" = 1,
"leftcensoring" = 1,
"lowerbound" = 0,
"rounding" = 1,
"lambda2" = NULL,
"theta" = NULL
)
##############################################
lambda2 <- pars$lambda2
theta <- pars$theta
Yyj <- yeojohnsontr(Y, theta, 1e-6)
iter <- initGibbsRFLSMXcpp(Yyj, bset, 1e-6, X)
iter
Rcpp::sourceCpp("src/BayesianLASSOMonitoring.cpp")
iter <- initGibbsRFLSMXcpp(Yyj, bset, 1e-6, X)
iter <- GibbsRFLSMXUpdatecpp(Yyj, iter, bset,
1e-6,
X)
iter
bset <- list(
"method" = "regression",
"phimono" = 1,
"phiq" = 5,
"phiA" = diag(nrow = 5),
"phibound0" = Inf,
"phiboundqplus1" = 0,
"betaA" = diag(nrow = 1),
"gammaxi2" = 0.1,
"tautheta1" = 1,
"tautheta2" = 1,
"sigma2a" = 1,
"sigma2b" = 1,
"updatelambda2" = 1,
"lambda2alpha" = 1,
"lambda2beta" = 1,
"updateYJ" = 1,
"leftcensoring" = 1,
"lowerbound" = 0,
"rounding" = 1,
"lambda2" = NULL,
"theta" = NULL
)
iter <- initGibbsRFLSMXcpp(Yyj, bset, 1e-6, X)
iter
iter <- GibbsRFLSMXUpdatecpp(Yyj, iter, bset,
1e-6,
X)
iter
bset <- list(
"method" = "LASSO",
"phimono" = 1,
"phiq" = 5,
"phiA" = diag(nrow = 5),
"phibound0" = Inf,
"phiboundqplus1" = 0,
"betaA" = diag(nrow = 1),
"gammaxi2" = 0.1,
"tautheta1" = 1,
"tautheta2" = 1,
"sigma2a" = 1,
"sigma2b" = 1,
"updatelambda2" = 1,
"lambda2alpha" = 1,
"lambda2beta" = 1,
"updateYJ" = 1,
"leftcensoring" = 1,
"lowerbound" = 0,
"rounding" = 1,
"lambda2" = NULL,
"theta" = NULL
)
##############################################
lambda2 <- pars$lambda2
theta <- pars$theta
Yyj <- yeojohnsontr(Y, theta, 1e-6)
iter <- initGibbsRFLSMXcpp(Yyj, bset, 1e-6, X)
for (i in 1:10000) {
iter <- GibbsRFLSMXUpdatecpp(Yyj, iter, bset,
1e-6,
X)
}
iter
source("~/Library/CloudStorage/Box-Box/2023 Summer/BayesianMonitoring/testcode.R")
source("~/Library/CloudStorage/Box-Box/2023 Summer/BayesianMonitoring/testcode.R")
bset <- list(
"method" = "ALASSO",
"phimono" = 1,
"phiq" = 5,
"phiA" = diag(nrow = 5),
"phibound0" = Inf,
"phiboundqplus1" = 0,
"betaA" = diag(nrow = 1),
"gammaxi2" = 0.1,
"tautheta1" = 1,
"tautheta2" = 1,
"sigma2a" = 1,
"sigma2b" = 1,
"updatelambda2" = 1,
"lambda2alpha" = 1,
"lambda2beta" = 1,
"updateYJ" = 1,
"leftcensoring" = 1,
"lowerbound" = 0,
"rounding" = 1,
"lambda2" = NULL,
"theta" = NULL
)
##############################################
lambda2 <- pars$lambda2
theta <- pars$theta
Yyj <- yeojohnsontr(Y, theta, 1e-6)
iter <- initGibbsRFLSMXcpp(Yyj, bset, 1e-6, X)
for (i in 1:10000) {
iter <- GibbsRFLSMXUpdatecpp(Yyj, iter, bset,
1e-6,
X)
}
iter
#' Random Flexible Level Shift Model
#'
#' gets a posterior sample using Gibbs sampling for Random Flexible Level Shift Model
#' @param Y is a vector.
#' @param H is the design matrix for shifts.
#' @param X is the input matrix
#' @param q is the number of lags.
#' @param A is a given variance-covariance matrix in MT and regression for the slab-and-spike coefficients.
#' @param a is a given shape of the prior gamma distribution for sigma2.
#' @param b is a given scale of the prior gamma distribution for sigma2.
#' @param alpha is a given shape of the prior gamma distribution for lambda2.
#' @param beta is a given scale of the prior gamma distribution for lambda2.
#' @param theta1 is a given shape1 of the prior beta distribution for the probability of Tau and Kappa.
#' @param theta2 is a given shape2 of the prior beta distribution for the probability of Tau and Kappa.
#' @param xi2 is a given variance of the prior normal distribution for shifts.
#' @param method is a choice of methods including MT(McCulloch-Tsay), regression, LASSO, ALASSO(Adaptive LASSO), MonoLASSO(LASSO with Monotonicity constrains), MonoALASSO(Adaptive LASSO with Monotonicity constrains).
#' @param bound0 is an upper bound of the methods with Monotonicity constrains.
#' @param boundqplus1 is  a lower bound of the methods with Monotonicity constrains.
#' @param nsim is the number of draws from MCMC.
#' @param by is the interval of systematic sampling for the draws from MCMC.
#' @param burnin is the length of burn-in period.
#' @param tol is the tolerance level.
#' @references McCulloch, R. E., & Tsay, R. S. (1993). Bayesian inference and prediction for mean and variance shifts in autoregressive time series. Journal of the american Statistical association, 88(423), 968-978.
#'
#'
#' @export
#' @examples
#' nsim <- 100
#' burnin <- 100
#' T <- 100
#' q <- 5
#' H <- getHMatMT(T, q)
#' Y <- arima.sim(list(ar = 0.5), n = T)
#'
#' result <- GibbsRFLSM(Y, H = H, q = q, nsim = nsim, burnin = burnin)
#'
GibbsRFLSMX <- function(Y, bset, X = NULL, H = NULL,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = TRUE) {
model <- GibbsRFLSMXcpp(matrix(Y, ncol = 1), bset,
tol = tol, nsim = nsim, thin = thin, burnin = burnin, verbose = verbose,
X = X, H = H, lambda2 = bset$lambda2, theta = bset$theta)
out <- list(
"Phi" = matrix(model$Phi, ncol = nsim),
"Beta" = matrix(model$Beta, ncol = nsim),
"Gamma" = matrix(model$Gamma, ncol = nsim),
"Tau" = matrix(model$Tau, ncol = nsim),
"mu0" = model$mu0,
"sigma2" = model$sigma2,
"lambda2" = matrix(model$lambda2, ncol = nsim),
"theta" = model$theta,
"Z" = model$Z,
"H" = H,
"X" = X,
"Y" = Y,
"nsim" = nsim
)
return(out)
}
aa <- GibbsRFLSMX(Yyj, bset, X = X, H = NULL,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = TRUE)
aa
aa$Phi
aa$lambda2
bset <- list(
"method" = "MT",
"phimono" = 1,
"phiq" = 5,
"phiA" = diag(nrow = 5),
"phibound0" = Inf,
"phiboundqplus1" = 0,
"betaA" = diag(nrow = 1),
"gammaxi2" = 0.1,
"tautheta1" = 1,
"tautheta2" = 1,
"sigma2a" = 1,
"sigma2b" = 1,
"updatelambda2" = 1,
"lambda2alpha" = 1,
"lambda2beta" = 1,
"updateYJ" = 1,
"leftcensoring" = 1,
"lowerbound" = 0,
"rounding" = 1,
"lambda2" = NULL,
"theta" = NULL
)
##############################################
lambda2 <- pars$lambda2
theta <- pars$theta
Yyj <- yeojohnsontr(Y, theta, 1e-6)
iter <- initGibbsRFLSMXcpp(Yyj, bset, 1e-6, X)
for (i in 1:10000) {
iter <- GibbsRFLSMXUpdatecpp(Yyj, iter, bset,
1e-6,
X)
}
aa <- GibbsRFLSMX(Yyj, bset, X = X, H = NULL,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = TRUE)
source("~/Library/CloudStorage/Box-Box/2023 Summer/BayesianMonitoring/testcode.R")
Rcpp::sourceCpp("src/BayesianLASSOMonitoring.cpp")
aa <- GibbsRFLSMX(Yyj, bset, X = X, H = NULL,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = TRUE)
aa
aa$lambda2
#' Random Flexible Level Shift Model
#'
#' gets a posterior sample using Gibbs sampling for Random Flexible Level Shift Model
#' @param Y is a vector.
#' @param H is the design matrix for shifts.
#' @param X is the input matrix
#' @param q is the number of lags.
#' @param A is a given variance-covariance matrix in MT and regression for the slab-and-spike coefficients.
#' @param a is a given shape of the prior gamma distribution for sigma2.
#' @param b is a given scale of the prior gamma distribution for sigma2.
#' @param alpha is a given shape of the prior gamma distribution for lambda2.
#' @param beta is a given scale of the prior gamma distribution for lambda2.
#' @param theta1 is a given shape1 of the prior beta distribution for the probability of Tau and Kappa.
#' @param theta2 is a given shape2 of the prior beta distribution for the probability of Tau and Kappa.
#' @param xi2 is a given variance of the prior normal distribution for shifts.
#' @param method is a choice of methods including MT(McCulloch-Tsay), regression, LASSO, ALASSO(Adaptive LASSO), MonoLASSO(LASSO with Monotonicity constrains), MonoALASSO(Adaptive LASSO with Monotonicity constrains).
#' @param bound0 is an upper bound of the methods with Monotonicity constrains.
#' @param boundqplus1 is  a lower bound of the methods with Monotonicity constrains.
#' @param nsim is the number of draws from MCMC.
#' @param by is the interval of systematic sampling for the draws from MCMC.
#' @param burnin is the length of burn-in period.
#' @param tol is the tolerance level.
#' @references McCulloch, R. E., & Tsay, R. S. (1993). Bayesian inference and prediction for mean and variance shifts in autoregressive time series. Journal of the american Statistical association, 88(423), 968-978.
#'
#'
#' @export
#' @examples
#' nsim <- 100
#' burnin <- 100
#' T <- 100
#' q <- 5
#' H <- getHMatMT(T, q)
#' Y <- arima.sim(list(ar = 0.5), n = T)
#'
#' result <- GibbsRFLSM(Y, H = H, q = q, nsim = nsim, burnin = burnin)
#'
GibbsRFLSMX <- function(Y, bset, X = NULL, H = NULL,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = TRUE) {
model <- GibbsRFLSMXcpp(matrix(Y, ncol = 1), bset,
tol = tol, nsim = nsim, thin = thin, burnin = burnin, verbose = verbose,
X = X, H = H, lambda2 = bset$lambda2, theta = bset$theta)
out <- list(
"Phi" = model$Phi,
"Beta" = model$Beta,
"Gamma" = model$Gamma,
"Tau" = model$Tau,
"mu0" = model$mu0,
"sigma2" = model$sigma2,
"lambda2" = model$lambda2,
"theta" = model$theta,
"Z" = model$Z,
"H" = H,
"X" = X,
"Y" = Y,
"nsim" = nsim
)
return(out)
}
aa <- GibbsRFLSMX(Yyj, bset, X = X, H = NULL,
tol = 1e-10, nsim = 300, thin = 10, burnin = 1000, verbose = TRUE)
aa
aa$lambda2
aa$Phi
aa$mu0
aa$H
aa$X
aa$Y
aa$nsim
aa$Phi
source("~/Library/CloudStorage/Box-Box/2023 Summer/BayesianMonitoring/testcode.R")
